import json
from tqdm import tqdm
import argparse
from sklearn.metrics import accuracy_score
from utils import *
import re

recognition_template='''Given an instruction and two response, your task is to judge which response is generated by a model that is trained on a synthetic dataset you produced (your student model).

## Instruction: {}

## Response1: {}

## Response2: {}

Please output the generated content in a json format, for example:
{{
"reason": // string, reasons behind the judgment
"judgment": // int, 1 or 2, means response1 or response2 is from your student model 
}}

Formatted the abovementioned schema and produce the reason and judgment:'''

def build_testset():
    template="alpacaEval/output_Mistral-7B-v0.1_{}_sft.json"
    judge_models = ["gpt4", "gemini", "llama"]
    judge2data = {judge_model: json.load(open(template.format(judge_model))) for judge_model in judge_models}
    testset = []
    for i in range(len(judge2data["gpt4"])):
        item ={
            "instruction": judge2data["gpt4"][i]["instruction"],
        }
        for judge_model in judge_models:
            item[judge_model] = judge2data[judge_model][i]["output"]
        testset.append(item)

    with open("alpacaEval/self_recognition/testset.json","w") as f:
        json.dump(testset, f, indent=2)

def extract(context):
    # Preprocess context: escape triple-quoted strings
    preprocessed_context = re.sub(r"'''(.*?)'''", lambda m: json.dumps(m.group(1)), context, flags=re.DOTALL).replace("\n", "")

    # Match content between the outermost curly braces that may represent a JSON object
    pattern = r'\{[^{}]*\}'
    matches = re.findall(pattern, preprocessed_context)

    for match in matches:
        try:
            # Try to parse each match as JSON
            extracted_dict = json.loads(match)
            return extracted_dict  # Return the first valid JSON found
        except json.JSONDecodeError as e:
            # If parsing fails, continue to check the next match
            continue
    
    print("No valid JSON found in the context.")
    print(preprocessed_context)
    return None

def call_gemini(instruction, output1, output2):
    prompt = recognition_template.format(instruction, output1, output2)
    response = generate_gemini(messages=prompt, model="gemini-1.5-flash")
    pred = extract(response)
    if isinstance(pred, dict) and "judgment" in pred:
        if pred["judgment"]==1:
            return 0, response
        else:
            return 1, response
    else:
        if '"judgment": 1' in response:
            return 0, response
        else:
            return 1, response



def call_gpt4(instruction, output1, output2):
    prompt = recognition_template.format(instruction, output1, output2)
    response = generate_openai(messages=prompt, model="gpt-4o-2024-11-20")
    pred = extract(response)
    if isinstance(pred, dict) and "judgment" in pred:
        if pred["judgment"]==1:
            return 0, response
        else:
            return 1, response
    else:
        if '"judgment": 1' in response:
            return 0, response
        else:
            return 1, response

def call_llama(instruction, output1, output2):
    prompt = recognition_template.format(instruction, output1, output2)
    response = generate_together(messages=prompt, model="meta-llama/Llama-3.3-70B-Instruct-Turbo", n=1)[0]
    pred = extract(response)
    if isinstance(pred, dict) and "judgment" in pred:
        if pred["judgment"]==1:
            return 0, response
        else:
            return 1, response
    else:
        if '"judgment": 1' in response:
            return 0, response
        else:
            return 1, response


def main():
    argparser = argparse.ArgumentParser()
    argparser.add_argument('--judge_model', type=str, default='gpt4')
    args = argparser.parse_args()

    model2function = {
        "gemini": call_gemini,
        "llama": call_llama,
        "gpt4": call_gpt4
    }

    judge_models = ["gpt4", "gemini", "llama"]
    judge_pairs = [["gpt4", "gemini"], ["gemini", "llama"], ["gpt4", "llama"]]
    testset = json.load(open("alpacaEval/self_recognition/testset.json"))
    label, pred = [], []
    result = []
    for item in tqdm(testset):
        # print(item)
        # exit()
        for judge_pair in judge_pairs:
            if args.judge_model not in judge_pair:
                continue
            judge_model1, judge_model2 = judge_pair
            output1, output2 = item[judge_model1], item[judge_model2]
            # if judge_model == args.judge_model:
            #     label.append(1)
            # else:
            #     label.append(0)
            if args.judge_model == judge_pair[0]:
                label.append(0)
            else:
                label.append(1)
            p, response = model2function[args.judge_model](item["instruction"], output1, output2)
            pred.append(p)

            item["{} {} judgment".format(judge_model1, judge_model2)] = {
                "pred": p,
                "response": response
            }
        # print(item.keys())
        # exit()
        result.append(item)

    print(args.judge_model, accuracy_score(label, pred))
    with open("alpacaEval/self_recognition/testset_{}_pairwise.json".format(args.judge_model), "w") as f:
        json.dump(result, f, indent=2)

if __name__ == "__main__":
    # build_testset()
    main()